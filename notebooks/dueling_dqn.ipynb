{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Dueling Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Motivation](#1)\n",
    "- [2 - Dueling network architecture](#2)\n",
    "- [3 - Advantages](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48d85a",
   "metadata": {},
   "source": [
    "Full paper: [Dueling Network Architectures for Deep Reinforcement Learning (2016)](https://arxiv.org/pdf/1511.06581.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128db14a",
   "metadata": {},
   "source": [
    "The motivation of the authors was to introduce a more suitable new neural network architecture for model-free reinforcement learning, which can be easily combined with existing and future algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49779e3",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 - Dueling network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fd97",
   "metadata": {},
   "source": [
    "The main idea behind the dueling network architecture is to separate the representation of state values and (state-dependent) action advantages. Instead of the common one stream network architecture, used to estimate the Q-values for each action, the dueling network architecture consists of two streams that represent the value and advantage functions, while sharing a common convolutional feature learning module. The two streams are then combined via a special aggregating layer to produce an estimate of the state-action value function Q.\n",
    "\n",
    "<img src=\"images/network_architecture.png\">\n",
    "<caption><center><font ><b>Figure 1</b>: Single stream Q-network (top) and dueling Q-network (bottom) </center></caption>\n",
    "    \n",
    "**Combination of the two streams**\n",
    "    \n",
    "To combine the two streams of fully-connected layers to output a Q estimate, we can't simply construct the aggregating module as follows:\n",
    "    \n",
    "$$Q(s, a, \\theta, \\alpha, \\beta) =  V(s; \\theta, \\beta) + A(s, a'; \\theta, \\alpha))$$\n",
    "    \n",
    "This is because the equation is unidentifiable in the sense that given Q, we cannot recover V and A uniquely. To address this issue, we can force the advantage function estimator to have zero advantage at the chosen action:\n",
    "    \n",
    "$$Q(s, a, \\theta, \\alpha, \\beta) =  V(s; \\theta, \\beta) + (A(s, a; \\theta, \\alpha) - \\max_{a' \\in \\vert A \\vert} A(s, a'; \\theta, \\alpha))$$\n",
    "    \n",
    "An alternative approach is to replace the max operator with an average:\n",
    "    \n",
    "$$Q(s, a, \\theta, \\alpha, \\beta) =  V(s; \\theta, \\beta) + (A(s, a; \\theta, \\alpha) - \\frac{1}{\\vert A \\vert}\\sum_{a'} A(s, a'; \\theta, \\alpha))$$\n",
    "    \n",
    "        \n",
    "In the equation above $\\theta$ denotes the parameters of the convolutional layers, while $\\alpha$ and $\\beta$ are the parameters of the two streams of fully-connected layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d65b98",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3 - Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719ff0e",
   "metadata": {},
   "source": [
    "Through the separation of the representation of state values and (state-dependent) action advantages we can learn the state-value function more efficiently. In contrast to a single-stream architecture, where at each step only the value for one of the actions is updated, in the dueling network architecture the value stream V is updated at each step. This allocates more resources to the representation of the state values and leads to a better approximation.\n",
    "\n",
    "As shown in figure 2, the dueling network architecture performs better than the traditional Q-network architecture, especially in large action spaces.\n",
    "\n",
    "<img src=\"images/nb_actions.png\">\n",
    "<caption><center><font ><b>Figure 2</b>: Performance comparision with increasing number of actions </center></caption>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
